---
layout:     post
title:      "浅析DP、MC、TD的区别和联系"
subtitle:   "lets begin"
date:       2020-05-01
author:     "CT"
header-img: "img/blog-bg.jpg"
tags:
    - 机器学习
---
# 引言
- Dynamic Programming, Monte Carlo, Temporal Difference是强化学习过程中最基础的三种算法，本文主要总结一下这三种方法的区别与联系；
- 强化学习模型本质上是一个随机过程，可以用概率图模型来描述，就像 HMM 可以使用有向图来描述，马尔可夫网可以使用无向图来描述，强化学习对应的图模型是Finite Markov Decision Process（MDP），如下图（也被称作智能体-环境交互模型）：
![img](/img/MC_1.png)
- 强化学习主要分为两步工作，第一步，预测每个状态的value function或Q function，第二步，根据value function或Q function生成Policy；
# Dynamic Programming
- MDP模型已知，使用**贝尔曼方程**进行迭代求解**value function**：
$$V_\pi(s)=\sum_a \pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma V_\pi(s')]$$
![img](/img/MC_2.png)
- 学习算法一般使用**GPI**（Generalized Policy Iteration），及**policy evaluation**和**policy improvement**同时进行，最终找到 **optimal value function**和**optimal policy**。**policy evaluation**过程使用的是bootstrap方法，向前传播；
![img](/img/MC_3.png)
# Monte Carlo 回合更新
- MDP模型未知，通过回合制（episode）**采样**来计算**value function**
![img](/img/MC_4.png)
- 每遍历完一个episode，就进行Q function更新：
$$Q(s,a)=Q(s,a)+\alpha [E(R(s,a))-Q(s,a)]$$
或value function更新：
$$V(s)=V(s)+\alpha [E(R(s))-V(s)]$$
$E(R(s,a))$表示一个episode内的$(s,a)$对应的回报的期望，$E(R(s))$ 表示一个episode内的 $s$ 对应的回报的期望，$\alpha$表示学习率；
- 例如：On-policy First-visit MC control algorithm
![img](/img/MC_5.png)
# Temporal Difference 单步更新
- MDP模型未知，通过回合制（episode）**采样**来计算**value function**，与MC的区别是，每执行完episode中的一步就进行更新；
- 更新方法：
$$V(s_t)=V(s_t)+\alpha[r_t+\gamma V(s_{t+1})]$$
- TD方法结合了 **DP** 的**bootstrap**和 **MC** 的**sampling**
- 典型算法：Sarsa(On-policy TD Control)、Q-Learning(Off-policy TD Control)、Sarsa($\lambda$)等

# 总结
- **TD**结合了**MC**的**sampling**方法和**DP**的**bootstrap**方法，是空间复杂度和时间复杂度都最低的算法，是大型状态空间问题的唯一解决方案；
- **MC**收敛于样本的**无偏估计**，**TD**收敛于样本的**确定等价估计**；