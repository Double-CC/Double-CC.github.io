---
layout:     post
title:      "逻辑回归LR推导"
subtitle:   "lets begin"
date:       2020-05-06
author:     "CT"
header-img: "img/blog-bg.jpg"
tags:
    - 机器学习
---
# 1 问题定义：
- 给定数据集$\{x_1,x_2, ...,x_m\}$和标签$\{y_1,y_2, ...,y_m\}$，训练一个模型，使得输入新的 $x$，输出对应的标签值。这里数据集 $x_i\in R^n$，标签 $y_i\in\{0,1\}$；
# 2 建立判别模型
- 建立一个判别模型，输入数据 $x$，输出该数据被分类成每个类别的概率；
- 对于二分类问题，我们使用sigmoid函数来建立判别模型：
$$p(y=1|x,\theta)=\sigma(\theta^Tx)$$
$$p(y=0|x,\theta)=1-\sigma(\theta^Tx)$$
# 3 建立似然函数：
- 对于所有数据集，建立似然函数：
$$L(\theta)=\prod_{i=1}^mp(y_i=1|x,\theta)^{y_i}p(y_i=0|x,\theta)^{1-y_i}$$
- 对数似然：
$$lnL(\theta)=\sum_{i=1}^m[y_iln\sigma(\theta^Tx_i)+(1-y_i)ln(1-\sigma(\theta^Tx_i))]$$
- 求导：
$$\frac{\alpha lnL(\theta)}{\alpha \theta}=\sum_{i=1}^mx_i[y_i-\sigma(\theta^Tx_i)]$$
- 令导数等于0，就得到了$\theta$的极大似然估计，但为了方便求解，这里使用梯度下降法，因此需要建立损失函数。
# 4 交叉熵损失函数
- 我们基于对数似然来定义交叉熵损失函数，及**极大化似然**就等价于**最小化交叉熵**：
$$cost(\theta)=-\frac{1}{m}lnL(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_iln\sigma(\theta^Tx_i)+(1-y_i)ln(1-\sigma(\theta^Tx_i))]$$
- 损失函数梯度：
$$\bigtriangledown cost(\theta)=-\frac{1}{m}\sum_{i=1}^mx_i[y_i-\sigma(\theta^Tx_i)]$$

# 5 为什么不使用最小二乘损失函数（LSE）？
- LSE损失函数：
$$cost_{LSE}(\theta)=\frac{1}{2m}\sum_{i=1}^m[y_i-\sigma(\theta^Tx_i)]^2$$
- LSE损失函数梯度：
$$\bigtriangledown cost_{LSE}(\theta)=\frac{1}{m}\sum_{i=1}^m[\sigma(\theta^Tx_i)-y_i]\sigma'(\theta^Tx_i)x_i$$
- 由于存在$\sigma'(\theta^Tx_i)$项，会出现梯度消失问题，而交叉熵损失函数梯度不存在这个问题；
